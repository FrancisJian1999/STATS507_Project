import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
 

data1 = pd.read_csv("ML_x.csv", engine='python',on_bad_lines='skip')
data2 = pd.read_csv("US_GDP_growth.csv", engine='python',on_bad_lines='skip')
#[CASEID  YYYY     ICE  UNEMP  GOVT  RATEX  INCOME    INVAMT STL5]
data1 = data1[data1.INVAMT != 99999998]
data2 = data2.rename(columns={'year': 'YYYY'})
data1['YYYY'] = data1['YYYY'].astype(np.float64)
data1['INCOME'] = pd.to_numeric(data1['INCOME'], errors='coerce')
data1['STL5'] = pd.to_numeric(data1['STL5'], errors='coerce')
data2 = data2.assign(gdp_a = lambda x: (x['gdp'] /x['population']))
#print(data1.head())
#print(data2.head())
data3 = pd.merge(left=data1, right=data2, left_on='YYYY', right_on='YYYY')
data3=data3.dropna()
#print(data3.head())

X = data3[['ICE','UNEMP', 'GOVT', 'RATEX', 'INCOME','STL5']]
normalized_X=(X-X.mean())/X.std()
y = data3['gdp_a']
print(X.head())
print(normalized_X.head())

#print(data3.head())
#print(X.dtypes)
#print(data3.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_X_train, n_X_test, n_y_train, n_y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)

# Fit a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
linear_pred = linear_model.predict(X_test)  

n_linear_model = LinearRegression()
n_linear_model.fit(n_X_train, n_y_train)
n_linear_pred = n_linear_model.predict(n_X_test)  

# Fit a random forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(n_X_train, n_y_train)
rf_pred = rf_model.predict(n_X_test)

# Fit an XGBoost model
xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(n_X_train, n_y_train)
xgb_pred = xgb_model.predict(n_X_test)

# Calculate mean squared error for each model
linear_mse = mean_squared_error(y_test, linear_pred)
rf_mse = mean_squared_error(n_y_test, rf_pred)
xgb_mse = mean_squared_error(n_y_test, xgb_pred)

n_linear_mse = mean_squared_error(n_y_test, n_linear_pred)


# Print the results
print(f"Linear regression MSE: {linear_mse}")
print(f"Linear regression n_MSE: {n_linear_mse}")
print(f"Random forest MSE: {rf_mse}")
print(f"XGBoost MSE: {xgb_mse}")

#print(linear_model.coef_)
#print(n_linear_model.coef_)
y_xgb_pred = xgb_model.predict(n_X_train)
xgb_residuals = n_y_train - y_xgb_pred

n_y_pred = n_linear_model.predict(n_X_train)
n_residuals = n_y_train - n_y_pred

plt.figure(1)
plt.scatter(n_y_train, xgb_residuals)
plt.xlabel('Actual Values')
plt.ylabel('Residuals')

plt.figure(2)
plt.scatter(n_y_train, n_residuals)
plt.xlabel('Actual Values')
plt.ylabel('Residuals')
plt.show()


